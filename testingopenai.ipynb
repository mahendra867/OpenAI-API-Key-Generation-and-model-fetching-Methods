{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638e45ad-2b57-4f96-9f04-2e158ca5c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d1feb9-b356-4ef6-bb80-ea3fdeb1e430",
   "metadata": {},
   "source": [
    "# 1. What is OpenA1 API?\n",
    "- This OpenAI API has been designed to provide developers with seamless access to state of art, pre trained, artifical\n",
    "intelligence models like gpt-3 gpt-4 dall-e whisper, embeddings etc so by using this openai api you can integrate\n",
    "cutting edge ai capabilities into your applications regardless the progamming language.\n",
    "- So,the conclusion is by using this OpenAI API you can unlock the advance functionalities and you can enhane the\n",
    "intelligence and performance of your application.\n",
    "\n",
    "\n",
    "### Reason for using OpenAI API\n",
    "\n",
    "suppose lets say when we have to integrate the chat bot into our usecase application in this case as a beginner it is necessary to build a LLM model with chat bot capability by training it with huge amount of unstructured data because for training our own customized LLM according to our task its is computationally expensive becacuse for training LLM model it requires lots of resources like GPU and roughly for training customized LLM it costs around in crores so rather than spending the huge amount of money for training LLMs it would be better to use the benfit of OpenAI APIs just by fetching the API different model of task respected capability we dont need to train that LLM because OpenAI company already trained these LLM models and just by writting the logic of use case in the LLM we can get responses from LLMS according to our usecase  so by  using this OpenAI API you can unlock the advance functionalities and you can enhane the intelligence and performance of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab383d5-629d-4a1d-aa26-0b3f4eb9f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78919537-4a53-4e84-b8e5-16b79afda268",
   "metadata": {},
   "source": [
    "### Generating OpenAI API key\n",
    "\n",
    "#### Steps\n",
    "- open OPENAI API website  https://platform.openai.com/docs/overview\n",
    "- click on API keys\n",
    "- click on Create new secret key\n",
    "- choose you option\n",
    "- Give name of API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d421f20-c0f9-4f21-84ed-ae6efe82e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mykey1 =\"\" # give your generated API key here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2312080c-2f20-4016-8667-e21595d20b0c",
   "metadata": {},
   "source": [
    "### Call the OpenAI API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee91b3fc-2f16-4896-86d4-1738cd07c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key=mykey1  # now are giving the generated api key to the openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a9cb54-bc5c-4b12-b78b-4f58d7966d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to check the available pretrained LLM models of Open API \n",
    "all_models=openai.models.list()   # here we are fetching list of LLMS pretrained models which are available in OpenAI organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bea0c56d-c28e-4b52-a6bc-db27d5a84b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'),\n",
       " Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'),\n",
       " Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'),\n",
       " Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'),\n",
       " Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system'),\n",
       " Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'),\n",
       " Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system'),\n",
       " Model(id='babbage-002', created=1692634615, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system'),\n",
       " Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'),\n",
       " Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'),\n",
       " Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'),\n",
       " Model(id='davinci-002', created=1692634301, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'),\n",
       " Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aa476f3-ccb7-457a-83df-d41fa3f1e219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created</th>\n",
       "      <th>object</th>\n",
       "      <th>owned_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(id, whisper-1)</td>\n",
       "      <td>(created, 1677532384)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(id, dall-e-2)</td>\n",
       "      <td>(created, 1698798177)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(id, gpt-3.5-turbo-16k)</td>\n",
       "      <td>(created, 1683758102)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(id, tts-1-hd-1106)</td>\n",
       "      <td>(created, 1699053533)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(id, tts-1-hd)</td>\n",
       "      <td>(created, 1699046015)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(id, gpt-3.5-turbo-instruct-0914)</td>\n",
       "      <td>(created, 1694122472)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(id, gpt-3.5-turbo-instruct)</td>\n",
       "      <td>(created, 1692901427)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(id, text-embedding-3-small)</td>\n",
       "      <td>(created, 1705948997)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(id, tts-1)</td>\n",
       "      <td>(created, 1681940951)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(id, text-embedding-3-large)</td>\n",
       "      <td>(created, 1705953180)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(id, babbage-002)</td>\n",
       "      <td>(created, 1692634615)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(id, gpt-3.5-turbo-0125)</td>\n",
       "      <td>(created, 1706048358)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(id, tts-1-1106)</td>\n",
       "      <td>(created, 1699053241)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(id, dall-e-3)</td>\n",
       "      <td>(created, 1698785189)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(id, text-embedding-ada-002)</td>\n",
       "      <td>(created, 1671217299)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(id, davinci-002)</td>\n",
       "      <td>(created, 1692634301)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(id, gpt-3.5-turbo)</td>\n",
       "      <td>(created, 1677610602)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(id, gpt-3.5-turbo-1106)</td>\n",
       "      <td>(created, 1698959748)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id                created           object  \\\n",
       "0                     (id, whisper-1)  (created, 1677532384)  (object, model)   \n",
       "1                      (id, dall-e-2)  (created, 1698798177)  (object, model)   \n",
       "2             (id, gpt-3.5-turbo-16k)  (created, 1683758102)  (object, model)   \n",
       "3                 (id, tts-1-hd-1106)  (created, 1699053533)  (object, model)   \n",
       "4                      (id, tts-1-hd)  (created, 1699046015)  (object, model)   \n",
       "5   (id, gpt-3.5-turbo-instruct-0914)  (created, 1694122472)  (object, model)   \n",
       "6        (id, gpt-3.5-turbo-instruct)  (created, 1692901427)  (object, model)   \n",
       "7        (id, text-embedding-3-small)  (created, 1705948997)  (object, model)   \n",
       "8                         (id, tts-1)  (created, 1681940951)  (object, model)   \n",
       "9        (id, text-embedding-3-large)  (created, 1705953180)  (object, model)   \n",
       "10                  (id, babbage-002)  (created, 1692634615)  (object, model)   \n",
       "11           (id, gpt-3.5-turbo-0125)  (created, 1706048358)  (object, model)   \n",
       "12                   (id, tts-1-1106)  (created, 1699053241)  (object, model)   \n",
       "13                     (id, dall-e-3)  (created, 1698785189)  (object, model)   \n",
       "14       (id, text-embedding-ada-002)  (created, 1671217299)  (object, model)   \n",
       "15                  (id, davinci-002)  (created, 1692634301)  (object, model)   \n",
       "16                (id, gpt-3.5-turbo)  (created, 1677610602)  (object, model)   \n",
       "17           (id, gpt-3.5-turbo-1106)  (created, 1698959748)  (object, model)   \n",
       "\n",
       "                       owned_by  \n",
       "0   (owned_by, openai-internal)  \n",
       "1            (owned_by, system)  \n",
       "2   (owned_by, openai-internal)  \n",
       "3            (owned_by, system)  \n",
       "4            (owned_by, system)  \n",
       "5            (owned_by, system)  \n",
       "6            (owned_by, system)  \n",
       "7            (owned_by, system)  \n",
       "8   (owned_by, openai-internal)  \n",
       "9            (owned_by, system)  \n",
       "10           (owned_by, system)  \n",
       "11           (owned_by, system)  \n",
       "12           (owned_by, system)  \n",
       "13           (owned_by, system)  \n",
       "14  (owned_by, openai-internal)  \n",
       "15           (owned_by, system)  \n",
       "16           (owned_by, openai)  \n",
       "17           (owned_by, system)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(list(all_models),columns=[\"id\",\"created\",\"object\",\"owned_by\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1226a2-2d03-4e84-8324-75acc911d222",
   "metadata": {},
   "source": [
    "#### Different methods to use the pretrained LLm Models by passing API key\n",
    "\n",
    "1)  Using Pre trained LLM models by OpenAI Playground method\n",
    "2)  Using Pretrained LLM model by OpenAI chat completion and function Call method\n",
    "3)  **Now come to assistant one**\n",
    "\n",
    "**Retrieval-augmented generation (RAG):**  is an artificial intelligence (AI) framework that retrieves data from external sources of knowledge to improve the quality of responses. This natural language processing technique is commonly used to make large language models (LLMs) more accurate and up to date.\n",
    "\n",
    "**Code Interpreter:** Python programming environment within ChatGPT where you can perform a wide range of tasks by executing Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac228c3b-b43e-4c48-a9e2-391e57793558",
   "metadata": {},
   "source": [
    "###  Using Pre trained LLM models by OpenAI Playground method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079d96c-c1a4-467c-8930-509eb0c10a5b",
   "metadata": {},
   "source": [
    "#### Follow the steps to use OpenAI LLM pretrained model by OpenAI playground method\n",
    "\n",
    "#### open website OpenAI playground  = https://platform.openai.com/playground/chat?models=gpt-3.5-turbo-16k\n",
    "click on Chat \n",
    "\n",
    "Lets first check the Chat option \n",
    "By using this Chat option we can Test the different different prompts or questions and i can generate the output and i can test different different pretrained LLM models, so along with model we can find various parameters \n",
    "\n",
    "#### First lets divide the CHat option in 3 segments \n",
    "1) system\n",
    "2) user\n",
    "3) different parameters which are in right side\n",
    "\n",
    "\n",
    "#### System \n",
    "In System we are going to set behaviour of the system\n",
    "Example :- your are a helpful assistance  \n",
    "\n",
    "#### User\n",
    "Example :- How i can make money\n",
    "\n",
    "#### Model\n",
    "select the needed model\n",
    "\n",
    "\n",
    "#### Temperature\n",
    "Temperature value lies between 0 to 2 as if we increase the temperature value LLM pretrained model generates the output in creative               way if Temperature value is close to 0 then model gives the straight forword output\n",
    "\n",
    "#### Maximum length \n",
    "Here we can set the Token Length\n",
    "\n",
    "#### Stop Sequence\n",
    "\n",
    "#### Top \n",
    "controlling the likelyhood of diversity in generating output and values likes between 0 to 1\n",
    "\n",
    "#### Frequence Penality \n",
    "it helps us to set the frequency of generating the repeated tokens or words in our output\n",
    "\n",
    "\n",
    "After setting all the parameters just click on Run option selected pre trained LLM model generates the output of the given prompt\n",
    "\n",
    "\n",
    "#### View code\n",
    "Then click on view code by copying that entire python code by utilizing the entire python code , if we have done the entire setup in our system we can directly hit or call the openAI API and we can use that selected GPT 3.5 Turbo model into our setuped application\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374bc9ea-99c1-48bf-b7ae-316ecea9b2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfbfa275-8331-4f57-8896-a7bb4789665d",
   "metadata": {},
   "source": [
    "### Using Pretrained LLM model by OpenAI chat completion and function Call method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "015b3630-6319-444c-992b-402457ee5fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore this https://pypi.org/project/openai/  pip repository here OpenAI community continously update their OpenAI package so according to package version the chat completion and fucntion call method code defining gets changes according to time\n",
    "\n",
    "from openai import OpenAI  # here iam importing OpenAI library which the OpenAI community deployed the OpenAI package in pipy repository \n",
    "client= OpenAI(api_key=mykey1) # here iam passing the generated api key to the OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ac09305-36dd-430e-a4e5-9a80dd972993",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# by using chat completion API and fucntion calling method iam passing few parameters to get the output from the model which user asked \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m response\u001b[38;5;241m=\u001b[39m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# here iam using the pretrained LLm model is gpt-3.5-turbo\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# here iam sending the prompt to the model \u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# first we need to declare the role by giving user\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhow can i make money\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# this is the prompt we want to generate from our pretrained LLM model\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m                                        \u001b[49m\u001b[38;5;66;43;03m# here we can define the so many parameters other than role and content like temperature ,n maximum_tokens, frequencey penalty, presence penalty, Top P\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# it means what is max_tokens or words that my model should need to generate while generating the output for the user prompt\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# it means it can generate the responses of output in 3 different ways\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mahen\\anaconda_new\\envs\\testingopenAI\\lib\\site-packages\\openai\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mahen\\anaconda_new\\envs\\testingopenAI\\lib\\site-packages\\openai\\resources\\chat\\completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mahen\\anaconda_new\\envs\\testingopenAI\\lib\\site-packages\\openai\\_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1239\u001b[0m     )\n\u001b[1;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\mahen\\anaconda_new\\envs\\testingopenAI\\lib\\site-packages\\openai\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mahen\\anaconda_new\\envs\\testingopenAI\\lib\\site-packages\\openai\\_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\mahen\\anaconda_new\\envs\\testingopenAI\\lib\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mahen\\anaconda_new\\envs\\testingopenAI\\lib\\site-packages\\openai\\_base_client.py:1005\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1004\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\mahen\\anaconda_new\\envs\\testingopenAI\\lib\\site-packages\\openai\\_base_client.py:1053\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mahen\\anaconda_new\\envs\\testingopenAI\\lib\\site-packages\\openai\\_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1019\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1027\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1028\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# by using chat completion API and fucntion calling method iam passing few parameters to get the output from the model which user asked \n",
    "response=client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # here iam using the pretrained LLm model is gpt-3.5-turbo\n",
    "    messages=[              # here iam sending the prompt to the model \n",
    "    {\n",
    "        \"role\":\"user\",        # first we need to declare the role by giving user\n",
    "        \"content\":\"how can i make money\"   # this is the prompt we want to generate from our pretrained LLM model\n",
    "    }                                        # here we can define the so many parameters other than role and content like temperature ,n maximum_tokens, frequencey penalty, presence penalty, Top P\n",
    "            ],\n",
    "    max_tokens=150, # it means what is max_tokens or words that my model should need to generate while generating the output for the user prompt\n",
    "    n=3   # it means it can generate the responses of output in 3 different ways\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ffa16-44af-4914-899a-6bd0750616fa",
   "metadata": {},
   "source": [
    "I got error in output response because i didnot set the payment method in OpenAI website thats y i cannot able use those re trained models by generating API key by different methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e34e5-732f-4fc5-b520-bf69ff5ceea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(respone)  # as we can see here type of response we got it by this method chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605239fa-1e9c-41df-9fca-b98dd3272f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching pretrained LLm model generated response\n",
    "response.choices[0].message.content  # for accessing exact output message which my  gpt-3.5-turbo generated by chat completion API and fucntion calling method first we need to access the 0 index of choices and then in that 0 index we need to access the content of message for the prompt how can i make money "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e9b9dd-553f-4e8a-a483-ea888ef4028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[1].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785092da-57db-4759-b7b8-12618cafe150",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[2].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225cd9f-b950-49d6-8778-98075da5b5d5",
   "metadata": {},
   "source": [
    "### Pricing of LLM pretrained models of OpenAI organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32d476-9792-45a6-8bd9-993a45c65b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mainly the prices of pretrained LLm models is dependent on No of Tokens a Model can generate , so the more no of Tokens the model could generate the more price we need to pay and price factor somehow depends on how old the model is and what are the model capabilities \n",
    "\n",
    "https://openai.com/pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0080e-3ab4-40d3-878b-368f9177711a",
   "metadata": {},
   "source": [
    "### caluclating the No of Tokens which are generated by model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd883b7-bea5-496b-82d8-99ce759dfea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45812b02-b15b-4057-9d43-2594bf3a6bcb",
   "metadata": {},
   "source": [
    "### Types of Prompt\n",
    "1) Zero shot prompt = when we given the straight forward prompt question to our pretrained LLM model to generate output response on that straight forward prompt question is calle Zero shot prompt\n",
    "2) few shot prompt = when we given the  prompt or question which it consist of some task that needs to perform by our OpenAI pretrained LLM model to generate output response on some description then this prompt is called few shot prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5255cd-79eb-40b1-bf32-7d53bbf52d64",
   "metadata": {},
   "source": [
    "### Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5217c-00ef-4d59-a623-d509fa22e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416691c-f574-4b84-84ca-9cd8671c9082",
   "metadata": {},
   "source": [
    "### Funcation Call Method of OpenAI\n",
    "- It is very imp feature of OpenAI\n",
    "- By using Funcation Call we can perform mulitple tasks of OpenAI\n",
    "- Very basic task we can perform by this Function call is that we can format the pretrained OpenAI LLM model generated output in according to our usecase related desired formate\n",
    "- Some Advance Use of Function call ,  Lets say we have asked 1 question which the OpenAI model could not able to answer for that particular  question and lets say we have called 3rd party API or any sort of Plugins other than Open AI for fetching the pretrained LLM model then whatever the output we are getting so we can formate that particular output and we can append that  particular output into our conversation chain for that particular question prompt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06445101-7821-4f0c-8b9f-c63c343eda92",
   "metadata": {},
   "source": [
    "#### Task\n",
    "- we will desgin one description\n",
    "- we will desgin one prompt in that prompt we will mention the task which that OpenAI pretrained LLM model needs to perform on description by the method chat completion API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2a0a596-5834-4ede-bf3f-ac00cc3db9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here i have created one Prompt or question which it consist of collection of sentences of collection of tokens\n",
    "student_description = \"Mahendra is a student of computer science at LPU. He is an indian and has a 8 GPA. Mahendra is known for his programming skills and is an active member of the college's AI Club. He hopes to pursue a career in artificial intelligence after graduating.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50d0ad91-869e-487c-9b15-6bcfa4223902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mahendra is a student of computer science at LPU. He is an indian and has a 8 GPA. Mahendra is known for his programming skills and is an active member of the college's AI Club. He hopes to pursue a career in artificial intelligence after graduating.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aff422-463f-48e9-81f4-75869b54f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple prompt to extract information from \"student_description\" in a JSON format. so in the Final prompt i will give below prompt task to my chat gpt model to extract the below given information from the student description prompt\n",
    "prompt = f'''\n",
    "Please extract the following information from the given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "college\n",
    "grades\n",
    "club\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_description}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdecfb9f-60a7-45c1-9a0e-cc48965c6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ff3e5-6902-4f5b-ab30-40709629092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to pass the final prompt to the OpenAI GPT model first we need to import the class of OpenAI and assign object to the OpenAI class to perform the above task of final prompt\n",
    "\n",
    "from openai import OpenAI  # from openai module iam importing the OpenAI class \n",
    "client =OpenAI(api_key=mykey1) # in order to use the OpenAI class i need to assign the object to the class of OpenAI() , in order to make connectivity of pretrained LLM model i need to pass the OpenAI API Key inside that class of OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c85570-45d9-4ec6-ae97-23804c504b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with the help of object of OpenAI class we can use the pre trained LLm model by Chat completions method  by using chat completion API and fucntion calling method iam passing few parameters to get the output from the model which user asked \n",
    "response=client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # here iam using the pretrained LLm model is gpt-3.5-turbo\n",
    "    messages=[              # here iam sending the prompt to the model \n",
    "    {\n",
    "        \"role\":\"user\",        # first we need to declare the role by giving user\n",
    "        \"content\":prompt   # this is the prompt we want to generate from our pretrained LLM model\n",
    "    }                                        # here we can define the so many parameters other than role and content like temperature ,n maximum_tokens, frequencey penalty, presence penalty, Top P\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4dd23a-ecc6-4e9a-a94e-324c837f724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f081952-db7a-4761-aa25-ce273e1db509",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a373eff-581f-407b-af6d-694804f55975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally my pretrained chat gpt model extracted the some particular information from description and now we could able to load that in jason formate by below code\n",
    "import json\n",
    "json.loads(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c240c0-309c-44b6-bada-e9b92bea2e58",
   "metadata": {},
   "source": [
    "#### Till so far we have executed or generated output response for the 2 types of prompts by our OpenAI pretrained LLM model by the method of chat completion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d10b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c785ab4-d4aa-4dbf-a37e-94cfd1f13384",
   "metadata": {},
   "source": [
    "## Now its time to work the same thing on Funcation Call Method \n",
    "\n",
    "vist documentation of Function call Capability of OpenAI   = https://platform.openai.com/docs/guides/function-calling\n",
    "\n",
    "#### Lets understand the basic fucntion defining w.r.t OpenAI and understand the fucntion structure and fucntion calling by chat completion API method of OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179d002-6bf3-4896-a51f-5c9b02a7cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here below i defined basic function with respect to OpenAI function defining and i named function name as student_custom_function, so function defining is not like that we use def for defining function in python for OPen AI function defining is the below method, after completing the defining of function we need to pass that function to the chat completion API method  \n",
    "student_custom_function = [\n",
    "    {\n",
    "        'name': 'extract_student_info',\n",
    "        'description': 'Get the student information from the body of the input text',  # here we assign the description which tells what is the main theme of function defining \n",
    "        'parameters': {    # inside the parameters i have defined the type which is object \n",
    "            'type': 'object',\n",
    "            'properties': {  # inside the properties i have deifined things name, college, grades, club\n",
    "                'name': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Name of the person'\n",
    "                },\n",
    "                'college': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The college name.'\n",
    "                },\n",
    "                'grades': {\n",
    "                    'type': 'integer',\n",
    "                    'description': 'CGPA of the student.'\n",
    "                },\n",
    "                'club': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'college club for extracurricular activities. '\n",
    "                }\n",
    "                \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603236e0-17ef-460a-a567-03c1a2795b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with the help of object of OpenAI class we can use the pre trained LLm model by Chat completions method  by using chat completion API and fucntion calling method iam passing few parameters to get the output from the model which user asked \n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt }],\n",
    "    functions=student_custom_function  # this is additional parameter i need to add in this chat completion method in order to execute the defined fucntion in certain structure way according to desired formate so this function is the conditions get applied on the given prompt thats get executed by our pretrained LLM model of OpenAI by chat completion method \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2.choices[0].message.function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13647141",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2.choices[0].message.function_call.arguments # so in the below output as we can see that output response got generated accoring to formate we mention in the function so when we are generating output response on prompt according to formate of function call in order to see the output of the pretrained LLM model how it is generated to defined function in desired way we need to use function_call.arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fee1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response2.choices[0].message.function_call.arguments) # to know what of datatype of output response which it is string type so stirng type is somehow difficult to read the generated output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3454379",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(response2.choices[0].message.function_call.arguments)  # getting the output generated on prompt according the formate of function format in json formate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f614bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(json.loads(response2.choices[0].message.function_call.arguments))  # to know what of datatype of output response when load the output by json format is easy to read the generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b277c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_description  # this is the prompt that we perform the customized output response generation according to function defined  and get excuted by chat completion API method "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5949d7e1",
   "metadata": {},
   "source": [
    "### performing the Function call of same function on 3 different prompts by chat completion API method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f89f9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mahendra is a student of computer science at LPU. He is an indian and has a 8 GPA. Mahendra is known for his programming skills and is an active member of the college's AI Club. He hopes to pursue a career in artificial intelligence after graduating.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 1\n",
    "student_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "432e461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 2\n",
    "student_description_two=\"krish naik is a student of computer science at IIT Mumbai. He is an indian and has a 9.5 GPA. krish is known for his programming skills and is an active member of the college's data science Club. He hopes to pursue a career in artificial intelligence after graduating.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "320cdb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 3\n",
    "student_description_three=\"sudhanshu kumar is a student of computer science at IIT bengalore. He is an indian and has a 9.2 GPA. krish is known for his programming skills and is an active member of the college's MLops Club. He hopes to pursue a career in artificial intelligence after graduating.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1759e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mahendra is a student of computer science at LPU. He is an indian and has a 8 GPA. Mahendra is known for his programming skills and is an active member of the college's AI Club. He hopes to pursue a career in artificial intelligence after graduating.\n",
      "krish naik is a student of computer science at IIT Mumbai. He is an indian and has a 9.5 GPA. krish is known for his programming skills and is an active member of the college's data science Club. He hopes to pursue a career in artificial intelligence after graduating.\n",
      "sudhanshu kumar is a student of computer science at IIT bengalore. He is an indian and has a 9.2 GPA. krish is known for his programming skills and is an active member of the college's MLops Club. He hopes to pursue a career in artificial intelligence after graduating.\n"
     ]
    }
   ],
   "source": [
    "# i have taken all the 3 prompts in list\n",
    "student_info=[student_description,student_description_two,student_description_three]\n",
    "for student in student_info:\n",
    "    print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df587287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "student_info = [student_description, student_description_two,student_description_three]\n",
    "for student in student_info:  # performed the for loop which it iterates over the every prompt which we passed in list and the prompts get executed by chat completion API method by model gpt-3.5-turbo  \n",
    "    response=client.chat.completions.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        messages = [{'role': 'user', 'content': student}],\n",
    "        functions = student_custom_function,\n",
    "        fucntion_call='auto'  # here i added the additional parameter by which we can resuse the same defined function for the 3 different prompts by the chat completion API method of fetching the pretraied LLm models\n",
    "    )\n",
    "\n",
    "    response = json.loads(response.choices[0].message.function_call.arguments)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e963cc3",
   "metadata": {},
   "source": [
    "### Appying the 2 different defined fucntions on different prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is my 2nd function in which we can define desire structure to generate output from different prompts by chat completion API method \n",
    "funtion_two=student_custom_function = [\n",
    "    {\n",
    "        'name': 'extract_student_info',\n",
    "        'description': 'Get the student information from the body of the input text',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'name': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'Name of the person'\n",
    "                },\n",
    "                'college': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'The college name.'\n",
    "                },\n",
    "                'grades': {\n",
    "                    'type': 'integer',\n",
    "                    'description': 'CGPA of the student.'\n",
    "                },\n",
    "                'club': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'college club for extracurricular activities. '\n",
    "                }\n",
    "                \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9423d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [student_custom_function[0],function_two[0]]\n",
    "student_info = [student_description, student_description_two,student_description_three]\n",
    "for student in student_info:\n",
    "    response =  client.chat.completions.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        messages = [{'role': 'user', 'content': student}],\n",
    "        functions = functions,  # here i have given the fucntions list which that list contains the different defined functions gets applied to 3 different prompts by for loop\n",
    "        function_call = 'auto'\n",
    "    )\n",
    "\n",
    "    response = json.loads(response.choices[0].message.function_call.arguments)\n",
    "    print(response)#import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74426d8d",
   "metadata": {},
   "source": [
    "### Advance Example of Function Call\n",
    "\n",
    "\n",
    "\n",
    "- Do you think that for the below mention content that my pretrained openai LLm model will generate the actual real time accurate information to the user being asked the staright forward content \n",
    "\n",
    "- The answer is no because OpenAI pretrained LLm model trained on data till year 2021 data so it does not generate the real time accurate information  for this types of prompts\n",
    "\n",
    "- so we need to make a chat bot application that should be capaible to answer to above types of questions too, and its get achieved by Advance function call feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d01984",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"When's the next flight from delhi to mumbai?\"  # this is the prompt iam asking my pretrained LLM model to generate response on the mentioned content \n",
    "    }   \n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbdfb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0].message.content  # in the below output we can confirm that OpenAI pretrained LLm model did not generated the response accuratly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97136f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_descriptions = [\n",
    "    {\n",
    "        \"name\": \"get_flight_info\",\n",
    "        \"description\": \"Get flight information between two locations\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"loc_origin\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The departure airport, e.g. DEL\",\n",
    "                },\n",
    "                \"loc_destination\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The destination airport, e.g. MUM\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"loc_origin\", \"loc_destination\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"When's the next flight from new delhi to mumbai?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f512a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": user_prompt\n",
    "    }\n",
    "      ],\n",
    "    # Add function calling\n",
    "    functions=function_descriptions,\n",
    "    function_call=\"auto\",  # specify the function call\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00967e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60410d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2.choices[0].message.function_call.arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58471a48",
   "metadata": {},
   "source": [
    "### assigment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae79a5",
   "metadata": {},
   "source": [
    "### call the real time api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb8de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta\n",
    "def get_flight_info(loc_origin, loc_destination):\n",
    "    \"\"\"Get flight information between two locations.\"\"\"\n",
    "\n",
    "    # Example output returned from an API or database\n",
    "    flight_info = {\n",
    "        \"loc_origin\": loc_origin,\n",
    "        \"loc_destination\": loc_destination,\n",
    "        \"datetime\": str(datetime.now() + timedelta(hours=2)),\n",
    "        \"airline\": \"KLM\",\n",
    "        \"flight\": \"KL643\",\n",
    "    }\n",
    "\n",
    "    return json.dumps(flight_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b1cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params=json.loads(response2.choices[0].message.function_call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdccf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(response2.choices[0].message.function_call.arguments).get(\"loc_origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(response2.choices[0].message.function_call.arguments).get('loc_destination')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8298699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = json.loads(response2.choices[0].message.function_call.arguments).get(\"loc_origin\")\n",
    "destination = json.loads(response2.choices[0].message.function_call.arguments).get(\"loc_destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2.choices[0].message.function_call.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(response2.choices[0].message.function_call.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(response2.choices[0].message.function_call.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb829dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(eval('2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33744495",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_function=eval(response2.choices[0].message.function_call.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3746fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight = chosen_function(**params)\n",
    "\n",
    "print(flight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c26cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2.choices[0].message.function_call.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8fc40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "    {\"role\": \"user\",\"content\": user_prompt},\n",
    "    {\"role\": \"function\", \"name\": response2.choices[0].message.function_call.name, \"content\": flight}\n",
    "      ],\n",
    "    # Add function calling\n",
    "    functions=function_descriptions,\n",
    "    function_call=\"auto\",  # specify the function call\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d46765",
   "metadata": {},
   "outputs": [],
   "source": [
    "response3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb229bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response3.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84244a",
   "metadata": {},
   "source": [
    "### Funtion Calling\n",
    "#### Learn how to connect large language models to external tools.\n",
    "\n",
    "## Langchain\n",
    "\n",
    "-  as we discussed this Langchain is the wrapper of OpenAI as same like keras is the wrapper of Tensorflow\n",
    "- till so far we requested the OpenAI API whatever the request we are making we are directly  getting results from OpenAI  , but when we use langchain when we request some prompt or question now rather than directly hitting the request the OpenAI we are asking our request to langchain which lanchain it will fetch the OpenAI to generate the results\n",
    "-  and the lanchain is not restricted to this useage it has many uses of lanchain and lanchain is very powerful application\n",
    "- And it is having a lots of uses it can call any third party API it can call any sort of a data resource it can it is having a power to read a different different documents it's a having a power to making a change to making a memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b93293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beaf54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the langchain it have different modules and classes so now we are going to import the OpenAI LLM module \n",
    "from langchain.llms import OpenAI  # as we discussed this Langchain is the wrapper of OpenAI as same like keras is the wrapper of Tensorflow  till so far we requested the OpenAI API whatever the request we are making we are directly  getting results from OpenAI  , but when we use langchain when we request some prompt or question now rather than directly hitting the request the OpenAI we are asking our request to langchain which lanchain it will fetch the OpenAI to generate the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client=OpenAI(openai_api_key=mykey1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero shot prompt\n",
    "prompt=\"can you tell me the total no of countries in Asia\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero shot prompting\n",
    "prompt2=\"can you tell me a capital of india?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.predict(prompt2).strip()  # strip is used to remove the /n in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8947dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3=\"​what exactly tokens , vector ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121583e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.predict(prompt3).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680da01",
   "metadata": {},
   "source": [
    "## Prompt Templates:\n",
    "\n",
    "- By using this Prompt Template we can construct the prompt or question based on input_varaible \n",
    "- A prompt template consist of question along with its input variable so based on the input_variable we pass the llm going to predict the prompt template just by passing the different input_variables we can get the different output context of the prompt templet w.r.t to that passed input_variable\n",
    "\n",
    "- by using any one of the method  format or from_template methods of prompttemplate class  we can perform this PromptTemplate functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "532a8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate # from prompt module of lanchain i imported PromptTemplate class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce05218",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name=PromptTemplate(\n",
    "    input_variables=[\"country\"],  # here i have first given the \"city\" and then i have changed the input_variable to \"country\" and \n",
    "    template=\"can you tell me the captial of {\"country\"} ?\"\n",
    ") # i have create a object for the class PromptTemplate to use PromptTemplate class operations which are defined in the backend code  so just by passing the arguments in the object  we can perform the operations by this class PromptTemplate so i have passed the input parameters like input_variable and template to the class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name.format(city=\"delhi\")  # now one of the parameter is being used by the one of the class method which is format() in that i passed the parameter like city then this formate method executed its operation by printing the template content with input_variable name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05632b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name.format(country=\"china\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed396d3",
   "metadata": {},
   "source": [
    "see the output it saying the capital of China is a buing so this prompt basically this prompt template will help you a lot whenever you are going to create any sort of application where you just required a single word from user\n",
    "\n",
    "Prompt Template main functionalty\n",
    "- By using this prompt template we can construct the Prompt based on a input variable now let's say you are going to create an application I can give you very uh good scenario now here is your application right here is what here is your application now you you have created this application by using the flas now here you are asking to the user just a city name just a city name or just a country name actually and based on that City or based on that country you want to provide a specific information and here you are using any sort of llm whether it's from hugging phas or open a now guys over here uh you don't want to be here actually you don't want that that your user is giving a entire prompt you just want to take a you just want to take a city name you just want to take a variable like we do in a python you know p in a python we we have a input function yes or no but and by using this input function we take a like input from the user and let's say we have to uh showcase the addition uh divide or maybe uh multiplication whatever on top of those input variable we can do it similarly over here let's say we are taking just a city name so by using this city name we can construct our prompt and that particular prompt we can pass to the llm and Leng chain gives you this particular functionality we don't have this thing inside open AI API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate.from_template(\"what is a good name for a compnay that makes a {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3=prompt.format(product=\"toys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.predict(prompt3).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dea64bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61646551",
   "metadata": {},
   "source": [
    "## Agent in Langchain\n",
    "\n",
    "Agent in langchain is used to call any Third party tool \n",
    "\n",
    "Agent means lets say i want to purchase one property and i have visted to builder officer where i will find an agent  of that property here i is the main person to purchase that particular property and agent collects the information from the property and agent will tell me about that property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04079d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt4=\"can you tell me who won the recent cricket world cup?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54667f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.predict(prompt4).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2283b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt5=\"can you tell me current GDP of india?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.predict(prompt5).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da68d61",
   "metadata": {},
   "source": [
    "As we can see in the above output we asked the langchain to hit the openAI  llm model to generate recent realtime information , but openai llm generated the information of 2019 world cup and GDP before jan 2022 , Because Open AI LLM model only have the knowledge before jan 2022 as it could not able to generate the output of current real time updated information thats why iam going to use the Langchain Agent function to call the 3rd party API which it will generate the output in real time according to the user being asked the question and the 3rd party API is serp API which it will call the google-search engine  to generate the real time information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff44597",
   "metadata": {},
   "source": [
    "## By using Agent iam going to extract the real time info, i am going to user serp api\n",
    "Now by using this serp api i will call google-search-engine and i will extract the information in a real time from google-search engine and we can also call the different apis like bing , linkdin ,wikipedia etc for extracting the content  \n",
    "\n",
    "\n",
    "This is the serp api website = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5495e21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results # iam going to install this google-search-results 3rd party api in the current using virtual environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3be88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5179d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61415ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd4f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb8ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f8b5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931324a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef7f5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8604f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692a65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce8c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dcc481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c424bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401b303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
